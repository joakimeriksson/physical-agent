# Jfokus 2026 - Lab Session

**Duration:** 3.5 hours (hands-on lab)

## Session Description

AI is moving out from the digital world into the physical world. In this hands-on lab, you'll get to explore how large language models (LLMs), multimodal AI, and edge intelligence can control real IoT devices and robotic systems. Together, we'll connect a robotic arm ("Candytron 4000"), cameras, microphones, and other sensors into an interactive setup where AI doesn't just chatâ€”it sees, listens, and acts. There will be candy handed out in this session!

## Key Components

- **LLMs** - Large Language Models with function calling/tool use
- **Multimodal AI** - Vision, speech, and action
- **Edge Intelligence** - Running AI on edge devices
- **Candytron 4000** - Robotic arm setup
- **Sensors** - Cameras, microphones, and other inputs

## Hardware Setup

### 1. Candytron 4000 (Niryo Ned 2)
- 6-axis collaborative robotic arm
- Gripper for candy manipulation
- External camera (candy detection / computer vision)
- Microphone (speech input)
- Speaker (TTS output)
- Table with assorted candy
- **Status: Code complete and working**

### 2. Reachy Mini (Pollen Robotics)
- Small humanoid robot
- Expressive head/face
- Dual arms with grippers

### 3. IKEA IoT Devices
- Smart home devices (lights, sensors, etc.)
- DIRIGERA hub / Zigbee-based
- Participants can control real home automation

## Software Components

- LLM with function calling (brain)
- Computer Vision / YOLO (eyes)
- Speech pipeline (ears/mouth)
- Robot control interface

## References

- MCP Agents: https://github.com/joakimeriksson/mcp-agents
- Candytron Demo: https://github.com/cluster1-arrowcolony/candytron-demo
